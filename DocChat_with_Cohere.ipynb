{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4540c5bb-0fb4-4e16-b747-b3636c5bca7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's import the required libraries from langchain\n",
    "import bs4\n",
    "from langchain import hub \n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7dd2cce6-b1e9-42a6-883d-38f4b7677ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"COHERE_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fedbe6f2-7773-413f-b91f-84ab86113ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the llm\n",
    "from langchain_cohere import ChatCohere\n",
    "\n",
    "llm = ChatCohere(model=\"command-r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1dff040a-9c26-49be-8d55-5441f56df0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the document using document loader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"https://arxiv.org/pdf/2103.15348.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b1ce5a3d-2e7d-4fd8-9d32-6771ec7ce777",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create splits of the document\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "840b3893-99a7-4403-b77a-71bbe6be0f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use an appropriate embedding model\n",
    "embeddings_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embeddings_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2842efa6-9ef2-4120-9cf4-690b45f00de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a vector store from document splits and the embedding model\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8cd31660-ad57-48c7-8f3c-fbccccd71b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "514bcaaa-159b-4319-9c24-e7b006f8aa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function to fomat the output content\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "efeecd82-2eb3-4753-9d8e-910aad071675",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Establish the rag chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "916e9c41-335d-4ca3-a7d4-086b50c20196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1zhshen@gmail.com) ,\\nHaibin Ling2( 12345@163.com ),\\nShuang Wang1( wshuang87@gmail.com) ,\\nJian Yang1( jyang@cs.princeton.edu) ,\\nand Wenchao Lu1(wlu@cs.princeton.edu)\\n1Princeton University, 2Tsinghua University\\nAbstract—Document image analysis is\\na challenging task due to the great vari-\\nety of document layouts and printing\\nstyles. Previous works often require\\ntedious feature engineering, which is\\nboth time-consuming and not robus\\nAnswer: The paper introduces LayoutParser, a toolkit for analysing document images. It addresses the challenges posed by diverse document layouts and printing styles, eliminating the need for time-consuming feature engineering. The method recognises vertical text, identifies column structures, and uses vertical positions to determine layout types.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"Summarize the paper in 10 sentences?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "097d0e6b-41f2-4e95-b7c8-127915c45fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LayoutParser has many advantages, including the ability to create large-scale and lightweight document digitization pipelines with ease. It also supports training customized layout models and community sharing of those models. This makes it a versatile tool with numerous potential applications.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What are some of the major advantages?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bfa6aad4-4fd6-495f-9fe4-ec6fa0449767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The paper being discussed appears to be eight rows long, although the exact dimensions are not stated. It seems to be written in a vertical style common in Japanese text. The paper's layout is complex, with variable width columns and row segmentation, which makes it difficult to pinpoint the exact length.\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"How long is the paper?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97884cf6-0471-486d-bbb1-151e5836bd7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"LayoutParser's main use case is to simplify the creation of document digitization pipelines, promoting reusability and reproducibility. These pipelines can be shared, discussed, and applied to solve specific problems related to document image analysis (DIA) tasks. LayoutParser also enables the community to share pre-trained models and various datasets.\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What is the majore use cases?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ecc3442-72cf-4295-b084-83ea59d4d226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Shen_Zejiang@hotmail.com)\\nYuliang Liu2(  liu.yuliang@gmail.com)* \\n1 Zhejiang University, Hangzhou, China \\n2 Microsoft Research, Redmond, WA, USA \\n\\nDeep learning is a subset of machine learning, which in turn is a subset of artificial intelligence. \\nIt enables computational models that are able to learn from data, \\nautomatically extracting relevant features for recognition or prediction tasks without being explicitly programmed.\\nAnswer: Deep learning is a branch of artificial intelligence that enables models to learn from data. These models can then be used for recognition or prediction, without being explicitly programmed for a specific task. This makes deep learning particularly useful for image analysis.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What is Deep learning?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70ae9301-83b6-4caf-b152-8d30f41811ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I couldn't find the answer to this based on the context provided. However, based on the larger context of the text, which appears to be a document about a toolkit for document image analysis (DIA), the purpose of life might be interpreted as the purpose of the 'life' or existence of the toolkit. In which case, the purpose could be interpreted as aiding and improving the process of digitisation.\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What is the purpose of life?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "900bda52-97db-4e63-bd40-c16b9256f47a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ShenZ17@pku.edu.cn )  ,  Meng Wang2(  mwang@cs.brown.edu  )\\n1School of Electronic Engineering and Computer Science, Peking University, Beijing, China ;  2Department of Computer Science, Brown University, Providence, RI, USA\\nAbstract—Document image analysis aims to develop automated systems that can understand and extract information from document images. Recent advances in deep learning have shown the great potential of end-to-end trainable models for this task. However, existing approaches require laborious annotations, and the models are usually tailored for specific tasks, which limits their generality and flexibility. To facilitate research on this topic, we propose LayoutParser, a code-through toolkit that unifies several state-of-the-art models for document image analysis. LayoutParser provides a wide range of pre-built components for document layout analysis, including text detection, text recognition, table detection, and image-text relationship modeling. With LayoutParser, users can easily construct various task-speciﬁc models by simply conﬁguring JSON ﬁles and require no additional code implementation. We hope LayoutParser can lower the barrier of deep learning based document image analysis research and help future work on this topic.\\n1. Introduction\\nAs the front cover of this issue indicates, document images appear in various forms, ranging from handwritten manuscripts and typewritten papers to newspaper articles and electronic scans. Document images carry rich information in both the textual content and visual layout. Automatic analysis of document images has practical signiﬁcance in many real-world applications, such as digital libraries [1], archive management [2], and information extraction from web articles [3]. Traditional methods for document image analysis are mostly based on hand-crafted features and complex pipelines, which are laborious to build and tune [4]. With the success of deep learning in many computer vision tasks [5], [6], end-to-end trainable models have shown superior performance in document image analysis [7]–[12]. However, existing deep learning based methods require a large amount of labeled data and are usually designed for speciﬁc tasks, which limits their generality and flexibility.\\n\\nAnswer the question using information from the above text. Intelligence is perceived as the ability of a system to understand and extract information from documents automatically. The recent advancements in deep learning have contributed greatly to this end. LayoutParser is a good example of a toolkit that unifies several state-of-the-art models for document image analysis.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"How do you percieve intelligence?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0a23bf8a-12ef-42d7-8a19-29111ad62747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ShenZ@nju.edu.cn ) , Yuxin Wu1,2 , Wenjie Li1,2 ,\\nShuang Chen1,2 , Jian Yang1 , Xuelong Li1 ,2 3\\n1 School of Computer Science and Engineering, Nanjing University, China\\n2 Jiangsu Key Laboratory of Big Data Analysis and Decision Making, Nanjing\\nUniversity, China\\n3 MOE Key Laboratory of Digital Publishing Technology, Peking University, China\\n\\n\\nAnswer in Japanese: ディープラーニングのモデル開発を簡素化するために努力しているにもかかわらず、現在利用可能なツールは、文書イメージ分析（DIA）の課題に最適化されていません。 このギャップを埋めるために、LayoutParserというオープンソースライブラリが導入されています。LayoutParserは、DLをDIA研究やアプリケーションに簡素化するために設計されたユニークなツールキットです。'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"Act as a Japanese Deep Learning Engineer and interpret the document for your collegues and answer in Japanese language\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fba5319-2e7b-4f42-a031-76d120d8688f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
